{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Variable Projection method for Separable non-linear problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to test and explore the variable projection method (Golub and Pereyra, 1973) on a simple exponential fitting problem. This kind of inverse problem falls into the set of separable non-linear problems; therefore, the variable-projection method (VarPro) can and should be applied. The analytical/modeling equation is represented by the following:\n",
    "\\begin{align}\n",
    "y_{i} = a \\exp(-b t_{i}),\n",
    "\\end{align}\n",
    "where $a$ and $b$ are the model parameters and $t_i$ indicates the $i-th$ time sample. We are going to minimize the simple $L_{2}$ difference between the model and the observed data. Hence, we can write the objective function as the following:\n",
    "\\begin{align}\n",
    "\\phi(a,b) = \\frac{1}{2} \\|\\mathbf{y}(a,b) - \\mathbf{y}_{obs} \\|_2^2.\n",
    "\\end{align}\n",
    "We notice that this objective function is quadratic with respect to parameter $a$ and non-linear with respect to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding library modules to PYTHONPATH\n",
    "import sys\n",
    "sys.path.append(\"../python\")\n",
    "import numpy as np\n",
    "#Inversion library-related modules\n",
    "import pyVector as Vec\n",
    "import pyOperator as Op\n",
    "import pyProblem as Prblm\n",
    "import pyVPproblem as VPprblm\n",
    "import pyStopper as Stopper\n",
    "import time\n",
    "#Solvers\n",
    "from pyLinearSolver import LCGsolver as LCG\n",
    "from pyNonLinearSolver import NLCGsolver as NLCG\n",
    "from pyNonLinearSolver import LBFGSsolver as LBFGS\n",
    "#Plotting library\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# %matplotlib inline\n",
    "params = {\n",
    "    'image.cmap': 'gray',\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 300,  # to adjust notebook inline plot size\n",
    "    'axes.labelsize': 14, # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize': 14,\n",
    "    'font.size': 14, \n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining non-linear modeling operator for symultaneous inversion\n",
    "Here we define the model vector as $\\mathbf{m}= [a,b]^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exponential_nl_sym(Op.Operator):\n",
    "\n",
    "    def __init__(self,model,t_vec):\n",
    "        \"\"\"y(a,b;t) = a * exp(-b*t) \"\"\"\n",
    "        self.setDomainRange(model,t_vec)\n",
    "        self.t_samples = t_vec.getNdArray()\n",
    "        return\n",
    "\n",
    "    def forward(self,add,model,data):\n",
    "        \"\"\"Forward non-linear\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        a = modelNd[0]\n",
    "        b = modelNd[1]\n",
    "        if(not add): data.zero()\n",
    "        dataNd += a * np.exp(-b*self.t_samples)\n",
    "        return\n",
    "    \n",
    "class exponential_jac_sym(Op.Operator):\n",
    "\n",
    "    def __init__(self,model,t_vec):\n",
    "        \"\"\"Jacobian matrix of the non-linear exponential for symultaneous inversion\"\"\"\n",
    "        self.setDomainRange(model,t_vec)\n",
    "        self.m0 = model.clone()\n",
    "        self.t_samples = t_vec.getNdArray()\n",
    "        return\n",
    "\n",
    "    def forward(self,add,model,data):\n",
    "        \"\"\"Forward jacobian\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        da = modelNd[0]\n",
    "        db = modelNd[1]\n",
    "        a0 = self.m0.getNdArray()[0]\n",
    "        b0 = self.m0.getNdArray()[1]\n",
    "        if(not add): data.zero()\n",
    "        dataNd += da * np.exp(-b0*self.t_samples) - a0 * self.t_samples * db* np.exp(-b0*self.t_samples)\n",
    "        return\n",
    "    \n",
    "    def adjoint(self,add,model,data):\n",
    "        \"\"\"Adjoint jacobian\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        da = modelNd[0]\n",
    "        db = modelNd[1]\n",
    "        a0 = self.m0.getNdArray()[0]\n",
    "        b0 = self.m0.getNdArray()[1]\n",
    "        if(not add): model.zero()\n",
    "        modelNd[0] += np.sum(dataNd*np.exp(-b0*self.t_samples))\n",
    "        modelNd[1] -= np.sum(dataNd*a0*self.t_samples*np.exp(-b0*self.t_samples))\n",
    "        return\n",
    "    \n",
    "    def set_m0(self,m0):\n",
    "        self.m0.copy(m0)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating observed data\n",
    "time_vec = Vec.vectorIC(np.linspace(0.,2.,200))\n",
    "a_true = 10.0\n",
    "b_true = 1.0\n",
    "true_m = Vec.vectorIC(np.array([a_true,b_true]))\n",
    "modeling_op = exponential_nl_sym(true_m,time_vec)\n",
    "data_true = time_vec.clone()\n",
    "modeling_op.forward(False,true_m,data_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "plt.plot(time_vec.getNdArray(),data_true.getNdArray())\n",
    "plt.grid()\n",
    "plt.title(\"Observed data points\")\n",
    "plt.xlabel(\"Observation time [s]\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "ax.autoscale(enable=True, axis='x', tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Jacobian of the non-linear operator\n",
    "modeling_op_jac = exponential_jac_sym(true_m,time_vec)\n",
    "non_lin_op = Op.NonLinearOperator(modeling_op,modeling_op_jac,modeling_op_jac.set_m0)\n",
    "#Instanciating non-linear problem\n",
    "L2_sym = Prblm.ProblemL2NonLinear(true_m,data_true,non_lin_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the objective function for plotting\n",
    "a_samples = np.linspace(0.0,30.0,500)\n",
    "b_samples = np.linspace(-1.0,6.0,200)\n",
    "obj_l2 = np.zeros((len(a_samples),len(b_samples)))\n",
    "model_test = Vec.vectorIC(np.array((0.0,0.0)))\n",
    "model_test_np = model_test.getNdArray()\n",
    "for ia,a_value in enumerate(a_samples):\n",
    "        for ib,b_value in enumerate(b_samples):\n",
    "                model_test_np[0] = a_value\n",
    "                model_test_np[1] = b_value\n",
    "                obj_l2[ia,ib]=L2_sym.get_obj(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the shape of the objective function\n",
    "fig,ax=plt.subplots()\n",
    "im=plt.imshow(obj_l2,vmin=0.0,vmax=2500,cmap='jet',extent=[-1.0,6.0,30.0,0.0])\n",
    "plt.xlabel(\"b [$s^{-1}$]\")\n",
    "plt.ylabel(\"a [Amplitude]\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_l2,levels=[1.0,10.0,100.0,500.0,1000.0,1500.0,2000.0,2500.0],extent=[-1.0,6.0,0.0,30.0],\n",
    "                 colors=\"black\",linewidths=(0.8,),linestyles=('--'))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label('$\\phi(a,b)$')\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "Stop  = Stopper.BasicStopper(niter=niter,tolr=1e-32,tolg=1e-32)\n",
    "NLCGsolver = NLCG(Stop,beta_type=\"FR\")\n",
    "model_init = model_test\n",
    "model_initNp = model_init.getNdArray()\n",
    "a_initial = 2.0\n",
    "b_initial = 0.0\n",
    "model_initNp[0] = a_initial\n",
    "model_initNp[1] = b_initial\n",
    "#Setting initial model\n",
    "_=L2_sym.get_obj(model_init)\n",
    "L2_sym.setDefaults()\n",
    "NLCGsolver.setDefaults(save_obj=True,save_model=True)\n",
    "t0 = time.time()\n",
    "NLCGsolver.run(L2_sym,verbose=True)\n",
    "print(\"Computational time %s s\"%(time.time()-t0))\n",
    "objNLCG = np.copy(NLCGsolver.obj)\n",
    "#Converting sampled points to arrays for plotting\n",
    "a_smpld=[]\n",
    "b_smpld=[]\n",
    "for iter in range(len(NLCGsolver.model)):\n",
    "    a_smpld.append(NLCGsolver.model[iter].getNdArray()[0])\n",
    "    b_smpld.append(NLCGsolver.model[iter].getNdArray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the optimization path\n",
    "fig,ax=plt.subplots()\n",
    "plt.scatter(b_smpld,a_smpld,color='red',s=50,marker=\"+\")\n",
    "plt.plot(b_smpld,a_smpld,\"--\",color='red')\n",
    "im=plt.imshow(obj_l2,vmin=0.0,vmax=2500,cmap='jet',extent=[-1.0,6.0,30.0,0.0])\n",
    "plt.xlabel(\"b [$s^{-1}$]\")\n",
    "plt.ylabel(\"a [Amplitude]\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_l2,levels=[1.0,10.0,100.0,500.0,1000.0,1500.0,2000.0,2500.0],extent=[-1.0,6.0,0.0,30.0],\n",
    "                 colors=\"black\",linewidths=(0.8,),linestyles=('--'))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label('$\\phi(a,b)$')\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSsolver = LBFGS(Stop)\n",
    "#Setting initial model\n",
    "_=L2_sym.get_obj(model_init)\n",
    "L2_sym.setDefaults()\n",
    "BFGSsolver.setDefaults(save_obj=True,save_model=True)\n",
    "t0 = time.time()\n",
    "BFGSsolver.run(L2_sym,verbose=True)\n",
    "objBFGS = np.copy(BFGSsolver.obj)\n",
    "print(\"Computational time %s s\"%(time.time()-t0))\n",
    "#Converting sampled points to arrays for plotting\n",
    "a_smpld=[]\n",
    "b_smpld=[]\n",
    "for iter in range(len(BFGSsolver.model)):\n",
    "    a_smpld.append(BFGSsolver.model[iter].getNdArray()[0])\n",
    "    b_smpld.append(BFGSsolver.model[iter].getNdArray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the optimization path\n",
    "fig,ax=plt.subplots()\n",
    "plt.scatter(b_smpld,a_smpld,color='red',s=50,marker=\"+\")\n",
    "plt.plot(b_smpld,a_smpld,\"--\",color='red')\n",
    "im=plt.imshow(obj_l2,vmin=0.0,vmax=2500,cmap='jet',extent=[-1.0,6.0,30.0,0.0])\n",
    "plt.xlabel(\"b [$s^{-1}$]\")\n",
    "plt.ylabel(\"a [Amplitude]\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_l2,levels=[1.0,10.0,100.0,500.0,1000.0,1500.0,2000.0,2500.0],extent=[-1.0,6.0,0.0,30.0],\n",
    "                 colors=\"black\",linewidths=(0.8,),linestyles=('--'))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label('$\\phi(a,b)$')\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to apply the VarPro method. Therefore, we reduce the number of model parameter to only $b$ because now $a$ is going to be function of $b$. In fact, we can write the new objective function as the following:\n",
    "\\begin{align}\n",
    "\\phi_{VarPro}(b) =  \\frac{1}{2} \\|\\mathbf{y}(a_{opt}(b),b) - \\mathbf{y}_{obs} \\|_2^2,\n",
    "\\end{align}\n",
    "where $a_{opt}(b)$ is given by:\n",
    "\\begin{align}\n",
    "a_{opt}(b) = \\frac{\\exp(-b\\mathbf{t})^{T}\\mathbf{y}_{obs}}{\\|\\exp(-b\\mathbf{t})\\|_2^2},\n",
    "\\end{align}\n",
    "which is the minimizer of the following quadratic problem for a fix $b$:\n",
    "\\begin{align}\n",
    "\\phi_{Lin}(a) =  \\frac{1}{2} \\|a \\exp(-b\\mathbf{t}) - \\mathbf{y}_{obs} \\|_2^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing optimal a for each b\n",
    "a_opt = []\n",
    "t_samples = time_vec.getNdArray()\n",
    "data_trueNp = data_true.getNdArray()\n",
    "for ib,b_value in enumerate(b_samples):\n",
    "    expb = np.exp(-b_value*t_samples)\n",
    "    a_opt.append(np.dot(expb,data_trueNp)/np.dot(expb,expb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the shape of the objective function and the curve along the optimal a\n",
    "fig,ax=plt.subplots()\n",
    "plt.plot(b_samples,a_opt,\"--\",color='red')\n",
    "im=plt.imshow(obj_l2,vmin=0.0,vmax=2500,cmap='jet',extent=[-1.0,6.0,30.0,0.0])\n",
    "plt.xlabel(\"b [$s^{-1}$]\")\n",
    "plt.ylabel(\"a [Amplitude]\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_l2,levels=[1.0,10.0,100.0,500.0,1000.0,1500.0,2000.0,2500.0],extent=[-1.0,6.0,0.0,30.0],\n",
    "                 colors=\"black\",linewidths=(0.8,),linestyles=('--'))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label('$\\phi(a,b)$')\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the objective function along curve of the optimal a\n",
    "obj_l2_vp = []\n",
    "for ib,b_value in enumerate(b_samples):\n",
    "    model_test_np[0] = a_opt[ib]\n",
    "    model_test_np[1] = b_value\n",
    "    obj_l2_vp.append(L2_sym.get_obj(model_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "plt.plot(b_samples,obj_l2_vp)\n",
    "plt.grid()\n",
    "plt.title(\"VarPro objective function\")\n",
    "plt.xlabel(\"b [$s^{-1}$]\")\n",
    "plt.ylabel(\"$\\phi_{VarPro}(b)$\")\n",
    "ax.autoscale(enable=True, axis='x', tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's solve the problem using the variable projection method. First, we need to create some operator to solve the linear problem when we fix the non-linear component of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exponential_lin_op(Op.Operator):\n",
    "\n",
    "    def __init__(self,model,t_vec):\n",
    "        self.setDomainRange(model,t_vec)\n",
    "        self.t_samples = t_vec.getNdArray()\n",
    "        self.nl_model = model.clone()\n",
    "        return\n",
    "\n",
    "    def forward(self,add,model,data):\n",
    "        \"\"\"Forward jacobian\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        b = self.nl_model.getNdArray()[0]\n",
    "        if(not add): data.zero()\n",
    "        dataNd += modelNd[0] * np.exp(-b*self.t_samples)\n",
    "        return\n",
    "    \n",
    "    def adjoint(self,add,model,data):\n",
    "        \"\"\"Adjoint jacobian\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        b = self.nl_model.getNdArray()[0]\n",
    "        if(not add): model.zero()\n",
    "        modelNd[0] += np.sum(dataNd*np.exp(-b*self.t_samples))\n",
    "        return\n",
    "    \n",
    "    def set_nl(self,nl_model):\n",
    "        \"\"\"Function to set the non-linear component of the operator\"\"\"\n",
    "        self.nl_model.copy(nl_model)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, let's create the operator to solve the non-linear problem in the reduced space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exponential_nl_vp(Op.Operator):\n",
    "\n",
    "    def __init__(self,model,t_vec):\n",
    "        \"\"\"y(a,b;t) = a * exp(-b*t) \"\"\"\n",
    "        self.setDomainRange(model,t_vec)\n",
    "        self.t_samples = t_vec.getNdArray()\n",
    "        self.lin_model = model.clone()\n",
    "        return\n",
    "\n",
    "    def forward(self,add,model,data):\n",
    "        \"\"\"Forward non-linear\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        a = self.lin_model.getNdArray()[0]\n",
    "        b = modelNd[0]\n",
    "        if(not add): data.zero()\n",
    "        dataNd += a * np.exp(-b*self.t_samples)\n",
    "        return\n",
    "    \n",
    "    def set_lin(self,lin_model):\n",
    "        \"\"\"Function to set the linear component of the operator\"\"\"\n",
    "        self.lin_model.copy(lin_model)\n",
    "        return\n",
    "\n",
    "class exponential_jac_vp(Op.Operator):\n",
    "\n",
    "    def __init__(self,model,t_vec):\n",
    "        \"\"\"Jacobian matrix of the non-linear exponential for VarPro inversion\"\"\"\n",
    "        self.setDomainRange(model,t_vec)\n",
    "        self.m0 = model.clone()\n",
    "        self.lin_model = model.clone()\n",
    "        self.t_samples = t_vec.getNdArray()\n",
    "        return\n",
    "\n",
    "    def forward(self,add,model,data):\n",
    "        \"\"\"Forward jacobian\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        db = modelNd[0]\n",
    "        a = self.lin_model.getNdArray()[0]\n",
    "        b0 = self.m0.getNdArray()[0]\n",
    "        if(not add): data.zero()\n",
    "        dataNd += - a * self.t_samples * db* np.exp(-b0*self.t_samples)\n",
    "        return\n",
    "    \n",
    "    def adjoint(self,add,model,data):\n",
    "        \"\"\"Adjoint jacobian\"\"\"\n",
    "        self.checkDomainRange(model,data)\n",
    "        modelNd = model.getNdArray()\n",
    "        dataNd = data.getNdArray()\n",
    "        a = self.lin_model.getNdArray()[0]\n",
    "        b0 = self.m0.getNdArray()[0]\n",
    "        if(not add): model.zero()\n",
    "        modelNd[0] -= np.sum(dataNd*a*self.t_samples*np.exp(-b0*self.t_samples))\n",
    "        return\n",
    "    \n",
    "    def set_m0(self,m0):\n",
    "        self.m0.copy(m0)\n",
    "        return\n",
    "    \n",
    "    def set_lin(self,lin_model):\n",
    "        \"\"\"Function to set the linear component of the operator\"\"\"\n",
    "        self.lin_model.copy(lin_model)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_init = Vec.vectorIC(np.array([a_initial]))\n",
    "b_init = Vec.vectorIC(np.array([b_initial]))\n",
    "#Creating non-linear operator\n",
    "expon_nl = exponential_nl_vp(b_init,time_vec)\n",
    "expon_nl.set_lin(a_init)\n",
    "#Creating VP operator\n",
    "expon_nl_jac = exponential_jac_vp(b_init,time_vec)\n",
    "exp_nl_op = Op.NonLinearOperator(expon_nl,expon_nl_jac,expon_nl_jac.set_m0)\n",
    "expon_lin = exponential_lin_op(a_init,time_vec)\n",
    "exp_vp_op = VPprblm.VpOperator(exp_nl_op,expon_lin,expon_lin.set_nl,expon_nl_jac.set_lin,\n",
    "                               set_lin=expon_nl.set_lin)\n",
    "#Instantiating VarPro problem object\n",
    "#Create solver for linear inverse problem\n",
    "LCGsolver = LCG(Stopper.BasicStopper(niter=10))\n",
    "VPproblem = VPprblm.ProblemL2VpReg(b_init,a_init,exp_vp_op,data_true,LCGsolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSsolver.setDefaults(save_obj=True,save_model=True)\n",
    "t0 = time.time()\n",
    "BFGSsolver.run(VPproblem,verbose=True)\n",
    "objVPBFGS = np.copy(BFGSsolver.obj)\n",
    "print(\"Computational time %s s\"%(time.time()-t0))\n",
    "#Converting sampled points to arrays for plotting\n",
    "b_smpld=[]\n",
    "for iter in range(len(BFGSsolver.model)):\n",
    "    b_smpld.append(BFGSsolver.model[iter].getNdArray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing corresponding optimal a\n",
    "a_smpld = []\n",
    "t_samples = time_vec.getNdArray()\n",
    "data_trueNp = data_true.getNdArray()\n",
    "for ib,b_value in enumerate(b_smpld):\n",
    "    expb = np.exp(-b_value*t_samples)\n",
    "    a_smpld.append(np.dot(expb,data_trueNp)/np.dot(expb,expb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the optimization path\n",
    "fig,ax=plt.subplots()\n",
    "plt.scatter(b_smpld,a_smpld,color='red',s=80,marker=\"+\")\n",
    "plt.plot(b_smpld,a_smpld,\"--\",color='red')\n",
    "plt.plot(b_samples,a_opt,\"--\",color='green')\n",
    "im=plt.imshow(obj_l2,vmin=0.0,vmax=2500,cmap='jet',extent=[-1.0,6.0,30.0,0.0])\n",
    "plt.xlabel(\"b [$s^{-1}$]\")\n",
    "plt.ylabel(\"a [Amplitude]\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_l2,levels=[1.0,10.0,100.0,500.0,1000.0,1500.0,2000.0,2500.0],extent=[-1.0,6.0,0.0,30.0],\n",
    "                 colors=\"black\",linewidths=(0.8,),linestyles=('--'))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label('$\\phi(a,b)$')\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting convergence curves\n",
    "fig,ax=plt.subplots()\n",
    "plt.plot(range(len(objNLCG)),np.log10(objNLCG/objNLCG[0]),label='NLCG')\n",
    "plt.plot(range(len(objBFGS)),np.log10(objBFGS/objBFGS[0]),label='BFGS')\n",
    "plt.plot(range(len(objVPBFGS)),np.log10(objVPBFGS/objVPBFGS[0]),label='VarPro')\n",
    "plt.grid()\n",
    "plt.title(\"Convergence curves\")\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"$log_{10}(\\phi_i/\\phi_0)$\")\n",
    "ax.legend()\n",
    "plt.xlim(0,50)\n",
    "_=plt.ylim(-30,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
