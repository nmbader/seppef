{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing non-linear solvers on the Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to set a user-defined objective function and minimize it using different solvers.\n",
    "The function understudy is the well-known convex Rosenbrock function. Its analytical form for the 2D case takes the follwing form:\n",
    "\\begin{equation}\n",
    "\\phi(x,y) = (1-x)^2 + 100 (y-x^2)^2,\n",
    "\\end{equation}\n",
    "in which the unique global minimum is at $x=y=1$. The global minimum is inside a long, narrow, parabolic-shaped flat valley. To find the valley is trivial. To converge to the global minimum, however, is difficult. Hence, this function represents a good testing case for any non-linear optimization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading necessary modules\n",
    "import sys\n",
    "sys.path.insert(0, \"../python\")\n",
    "import pyVector as Vec\n",
    "import pyOperator as Op\n",
    "import pyProblem as Prblm\n",
    "from pyStopper import BasicStopper as Stopper\n",
    "from pyStepper import ParabolicStep as StepperPar\n",
    "from pyStepper import CvSrchStep as StepperMT\n",
    "from pyNonLinearSolver import NLCGsolver as NLCG\n",
    "from pyNonLinearSolver import LBFGSsolver as LBFGS\n",
    "import numpy as np\n",
    "#Plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline\n",
    "params = {\n",
    "    'image.interpolation': 'nearest',\n",
    "    'image.cmap': 'gray',\n",
    "    'savefig.dpi': 300,  # to adjust notebook inline plot size\n",
    "    'axes.labelsize': 12, # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize': 12,\n",
    "    'font.size': 12, # was 10\n",
    "    'legend.fontsize': 12, # was 10\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the problem object. Our model vector is going to be $\\mathbf{m} = [x \\,, \\, y]^T$. Since the libary assumes that the objective function is written in terms of some residual vector (i.e., $\\phi(\\mathbf{r}(\\mathbf{m}))$, we will create a vector containing objective function as a single scalar value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rosenbrock_prblm(Prblm.Problem):\n",
    "\t\"\"\"\n",
    "\t   Rosenbrock function inverse problem\n",
    "\t   f(x,y) = (1 - x)^2 + 100*(y -x^2)^2\n",
    "\t   m = [x y]'\n",
    "\t   res = objective function value\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self,x_initial,y_initial):\n",
    "\t\t\"\"\"Constructor of linear problem\"\"\"\n",
    "\t\t#Setting the bounds (if any)\n",
    "\t\tsuper(Rosenbrock_prblm,self).__init__(None,None)\n",
    "\t\t#Setting initial model\n",
    "\t\tself.model  = Vec.vectorIC(np.array((x_initial,y_initial)))\n",
    "\t\tself.dmodel = self.model.clone()\n",
    "\t\tself.dmodel.zero()\n",
    "\t\t#Gradient vector\n",
    "\t\tself.grad=self.dmodel.clone()\n",
    "\t\t#Residual vector\n",
    "\t\tself.res = Vec.vectorIC(np.array((0.,)))\n",
    "\t\t#Dresidual vector\n",
    "\t\tself.dres=self.res.clone()\n",
    "\t\t#Setting default variables\n",
    "\t\tself.setDefaults()\n",
    "\t\tself.linear=False\n",
    "\t\treturn\n",
    "\n",
    "\tdef objf(self,model):\n",
    "\t\t\"\"\"Objective function computation\"\"\"\n",
    "\t\tm = model.getNdArray() #Getting ndArray of the model\n",
    "\t\tobj = self.res.arr[0]\n",
    "\t\treturn obj\n",
    "\n",
    "\tdef resf(self,model):\n",
    "\t\t\"\"\"Residual function\"\"\"\n",
    "\t\tm = model.getNdArray() #Getting ndArray of the model\n",
    "\t\tself.res.getNdArray()[0] = (1.0 - m[0])*(1.0 - m[0]) + 100.0 * (m[1] - m[0]*m[0]) * (m[1] - m[0]*m[0])\n",
    "\t\treturn self.res\n",
    "\n",
    "\tdef gradf(self,model,res):\n",
    "\t\t\"\"\"Gradient computation\"\"\"\n",
    "\t\tm = model.getNdArray() #Getting ndArray of the model\n",
    "\t\tself.grad.getNdArray()[0] = - 2.0 * (1.0 - m[0]) - 400.0 * m[0] * (m[1] - m[0]*m[0])\n",
    "\t\tself.grad.getNdArray()[1] = 200.0 * (m[1] - m[0]*m[0])\n",
    "\t\treturn self.grad\n",
    "\n",
    "\tdef dresf(self,model,dmodel):\n",
    "\t\t\"\"\"Linear variation of the objective function value\"\"\"\n",
    "\t\tm = model.getNdArray() #Getting ndArray of the model\n",
    "\t\tdm = dmodel.getNdArray() #Getting ndArray of the model\n",
    "\t\tself.dres.arr[0] = (- 2.0 * (1.0 - m[0]) - 400.0 * m[0] * (m[1] - m[0]*m[0]))* dm[0] + (200.0 * (m[1] - m[0]*m[0])) * dm[1]\n",
    "\t\treturn self.dres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation of the inverse problem and of the various solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting point for all the optimization problem\n",
    "x_init = -1.0\n",
    "y_init = -1.0\n",
    "#Testing solver on Rosenbrock function\n",
    "Ros_prob = Rosenbrock_prblm(x_init,y_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running any inversion, let's compute the objective function for different values of $x$ and $y$. This step will be useful when we want to plot the optimization path taken by the various tested algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the objective function for plotting\n",
    "x_samples = np.linspace(-1.5,1.5,1000)\n",
    "y_samples = np.linspace(3,-1.5,1000)\n",
    "obj_ros = Vec.vectorIC(np.zeros((len(x_samples),len(y_samples))))\n",
    "obj_ros_np = obj_ros.getNdArray()\n",
    "model_test = Vec.vectorIC(np.array((0.0,0.0)))\n",
    "model_test_np = model_test.getNdArray()\n",
    "for ix,x_value in enumerate(x_samples):\n",
    "\tfor iy,y_value in enumerate(y_samples):\n",
    "\t\tmodel_test_np[0] = x_value\n",
    "\t\tmodel_test_np[1] = y_value\n",
    "\t\tobj_ros_np[ix,iy]=Ros_prob.get_obj(model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test a non-linear conjugate-gradient method in which a parabolic stepper with three-point interpolation is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "Stop  = Stopper(niter=niter,tolr=1e-32,tolg=1e-32)\n",
    "NLCGsolver = NLCG(Stop)\n",
    "Ros_prob = Rosenbrock_prblm(x_init,y_init) #Resetting the problem\n",
    "NLCGsolver.setDefaults(save_obj=True,save_model=True)\n",
    "NLCGsolver.run(Ros_prob,verbose=True)\n",
    "#Converting sampled points to arrays for plotting\n",
    "x_smpld=[]\n",
    "y_smpld=[]\n",
    "for iter in range(len(NLCGsolver.model)):\n",
    "    x_smpld.append(NLCGsolver.model[iter].getNdArray()[0])\n",
    "    y_smpld.append(NLCGsolver.model[iter].getNdArray()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the optimization path taken by the algorithm, which converged to the global minimum in 199 iterations using a parabolic stepper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "# im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[-1.5,1.5,-1.0,3.0])\n",
    "plt.scatter(x_smpld,y_smpld,color='red',s=50,marker=\"+\")\n",
    "plt.plot(x_smpld,y_smpld,\"--\",color='red')\n",
    "im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[1.5,-1.5,-1.5,3.0])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_ros_np.T,levels=[0.05,0.1,0.5,2,10,50,125,250,500,1000],extent=[-1.5,1.5,3.0,-1.5],\n",
    "                 colors=\"white\",linewidths=(0.8,),linestyles=('--'))\n",
    "plt.gca().invert_xaxis()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second test, we will test the steppest-descent approach using the same stepper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLSDsolver = NLCG(Stop,beta_type=\"SD\")\n",
    "Ros_prob = Rosenbrock_prblm(x_init,y_init) #Resetting the problem\n",
    "NLSDsolver.setDefaults(save_obj=True,save_model=True)\n",
    "NLSDsolver.run(Ros_prob,verbose=True)\n",
    "#Converting sampled points to arrays for plotting\n",
    "x_smpld=[]\n",
    "y_smpld=[]\n",
    "for iter in range(len(NLSDsolver.model)):\n",
    "    x_smpld.append(NLSDsolver.model[iter].getNdArray()[0])\n",
    "    y_smpld.append(NLSDsolver.model[iter].getNdArray()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again plot the optimization path. In this case, the algorithm finds only falls close to the vicinity of the global minimum but does not reach even after 1000 iteration. In the figure below, we can see that the algorithm is sampling most of the objective function within the parabolic valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "# im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[-1.5,1.5,-1.0,3.0])\n",
    "plt.scatter(x_smpld,y_smpld,color='red',s=50,marker=\"+\")\n",
    "plt.plot(x_smpld,y_smpld,\"--\",color='red')\n",
    "im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[1.5,-1.5,-1.5,3.0])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_ros_np.T,levels=[0.05,0.1,0.5,2,10,50,125,250,500,1000],extent=[-1.5,1.5,3.0,-1.5],\n",
    "                 colors=\"white\",linewidths=(0.8,),linestyles=('--'))\n",
    "plt.gca().invert_xaxis()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third test, let's apply the BFGS algorithm to find the function's global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParabStep = StepperPar() #Again we use the same parabolic stepper as before\n",
    "BFGSsolver = LBFGS(Stop,stepper=ParabStep)\n",
    "Ros_prob = Rosenbrock_prblm(x_init,y_init) #Resetting the problem\n",
    "BFGSsolver.setDefaults(save_obj=True,save_model=True)\n",
    "BFGSsolver.run(Ros_prob,verbose=True)\n",
    "#Converting sampled points to arrays for plotting\n",
    "x_smpld=[]\n",
    "y_smpld=[]\n",
    "for iter in range(len(BFGSsolver.model)):\n",
    "    x_smpld.append(BFGSsolver.model[iter].getNdArray()[0])\n",
    "    y_smpld.append(BFGSsolver.model[iter].getNdArray()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has precisely reached the global minimum in 24 iterations. We can clearly see that it is able to find an approximation of the local curvature of the objective function. In fact, it needs to sample very few points within the parabolic-shaped valley. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "# im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[-1.5,1.5,-1.0,3.0])\n",
    "plt.scatter(x_smpld,y_smpld,color='red',s=50,marker=\"+\")\n",
    "plt.plot(x_smpld,y_smpld,\"--\",color='red')\n",
    "im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[1.5,-1.5,-1.5,3.0])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_ros_np.T,levels=[0.05,0.1,0.5,2,10,50,125,250,500,1000],extent=[-1.5,1.5,3.0,-1.5],\n",
    "                 colors=\"white\",linewidths=(0.8,),linestyles=('--'))\n",
    "plt.gca().invert_xaxis()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "ax.set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test again the BFGS method but this time employing the line-search algorithm proposed by More and Thuente (1994). Their line-search method uses a backeting approach in which the strong Wolfe conditions are verified for the tested point. In this case, if these conditons are met, then the method was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSsolver = LBFGS(Stop)\n",
    "Ros_prob = Rosenbrock_prblm(x_init,y_init) #Resetting the problem\n",
    "BFGSsolver.setDefaults(save_obj=True,save_model=True)\n",
    "BFGSsolver.run(Ros_prob,verbose=True)\n",
    "#Converting sampled points to arrays for plotting\n",
    "x_smpld=[]\n",
    "y_smpld=[]\n",
    "for iter in range(len(BFGSsolver.model)):\n",
    "    x_smpld.append(BFGSsolver.model[iter].getNdArray()[0])\n",
    "    y_smpld.append(BFGSsolver.model[iter].getNdArray()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the algorithm has reached the global minimum in 36 iterations. However, since we employed a different stepping method, in which no parabolic interpolation is used during the optimization, the algorithm had to perfom only 41 objective function evaluations as opposed to 73 necessary by the BFGS method when the parabolic stepper was the line-search algorithm of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "# im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[-1.5,1.5,-1.0,3.0])\n",
    "plt.scatter(x_smpld,y_smpld,color='red',s=50,marker=\"+\")\n",
    "plt.plot(x_smpld,y_smpld,\"--\",color='red')\n",
    "im=plt.imshow(obj_ros_np.T,cmap='jet',vmin=0.0,vmax=600,extent=[1.5,-1.5,-1.5,3.0])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "cs = plt.contour(obj_ros_np.T,levels=[0.05,0.1,0.5,2,10,50,125,250,500,1000],extent=[-1.5,1.5,3.0,-1.5],\n",
    "                 colors=\"white\",linewidths=(0.8,),linestyles=('--'))\n",
    "plt.gca().invert_xaxis()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(im, cax=cax)\n",
    "ax.set_aspect('auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
